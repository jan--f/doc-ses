<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage.salt.cluster">
 <title>&salt; Cluster Administration</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  After you deploy &ceph; cluster, you will probably need to perform several
  modifications to it occasionally. These include adding or removing new nodes,
  disks, or services. This chapter describes how you can achieve these
  administration tasks.
 </para>
 <sect1 xml:id="salt.adding.nodes">
  <title>Adding New Cluster Nodes</title>

  <para>
   The procedure of adding new nodes to the cluster is almost identical to the
   initial cluster node deployment described in
   <xref linkend="ceph.install.saltstack"/>:
  </para>

  <procedure>
   <step>
    <para>
     Install &cephos; on the new node, configure its network setting so that it
     resolves the &smaster; host name correctly, and install the
     <systemitem>salt-minion</systemitem> package:
    </para>
<screen>&prompt.sminion;zypper in salt-minion</screen>
    <para>
     If the &smaster;'s host name is different from <literal>salt</literal>,
     edit <filename>/etc/salt/minion</filename> and add the following:
    </para>
<screen>master: <replaceable>DNS_name_of_your_salt_master</replaceable></screen>
    <para>
     If you performed any changes to the configuration files mentioned above,
     restart the <systemitem>salt.minion</systemitem> service:
    </para>
<screen>&prompt.sminion;systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Accept all salt keys on the &smaster;:
    </para>
<screen>&prompt.smaster;salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Verify that <filename>/srv/pillar/ceph/deepsea_minions.sls</filename>
     targets the new &sminion; as well. Refer to
     <xref
      linkend="ds.minion.targeting.name"/> of
     <xref linkend="ds.depl.stages"/> for more details.
    </para>
   </step>
   <step>
    <para>
     Run the preparation stage. It synchronizes modules and grains so that the
     new minion can provide all the information &deepsea; expects:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>
   </step>
   <step>
    <para>
     Run the discovery stage. It will write new file entries in the
     <filename>/srv/pillar/ceph/proposals</filename> directory, where you can
     edit relevant .yml files:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Optionally, change
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> if the newly
     added host does not match the existing naming scheme. For details, refer
     to <xref linkend="policy.configuration"/>.
    </para>
   </step>
   <step>
    <para>
     Run the configuration stage. It reads everything under
     <filename>/srv/pillar/ceph</filename> and updates the pillar accordingly:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
    <para>
     Pillar stores data which you can access with the following command:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> pillar.items</screen>
   </step>
   <step>
    <para>
     The configuration and deployment stages include newly added nodes:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.3
&prompt.smaster;salt-run state.orch ceph.stage.4</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt.adding.services">
  <title>Adding New Roles to Nodes</title>

  <para>
   You can deploy all types of supported roles with &deepsea;. See
   <xref linkend="policy.role.assignment"/> for more information on supported
   role types and examples of matching them.
  </para>

  <tip>
   <title>Required and Optional Roles and Stages</title>
   <para>
    Generally, we recommend running all deployment Stages 0 to 5 when adding a
    new role to a cluster node. To save some time, you can skip Stages 3 or 4,
    depending on which type of role you intend to deploy. Whereas OSD and MON
    roles include core services and are required by &ceph;, other roles such as
    &rgw; are optional. &deepsea; deployment stages are hierarchical: whereas
    Stage 3 deploys core services, Stage 4 deploys the optional ones.
   </para>
   <para>
    Therefore, you need to run Stage 3 when deploying core roles, such as MON
    on an existing OSD node, and you may skip Stage 4.
   </para>
   <para>
    Similarly, you can skip Stage 3 when deploying optional services such as
    &rgw;, but you need to run Stage 4 in that case.
   </para>
  </tip>

  <para>
   To add a new service to an existing node, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Adapt <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> to match
     the existing host with a new role. For more details, refer to
     <xref
      linkend="policy.configuration"/>. For example, if you need to
     run a &rgw; on a MON node, the line is similar to:
    </para>
<screen>role-rgw/xx/x/example.mon-1.sls</screen>
   </step>
   <step>
    <para>
     Run Stage 2 to update the pillar:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
   </step>
   <step>
    <para>
     Run Stage 3 to deploy core services, or Stage 4 to deploy optional
     services. Running both stages does not hurt.
    </para>
   </step>
  </procedure>

  <tip>
   <para>
    When adding an OSD to the existing cluster, bear in mind that the cluster
    will be rebalancing for some time afterward. To minimize the rebalancing
    periods, we recommend adding all the OSDs you intend to add at the same
    time.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="salt.node.removing">
  <title>Removing and Reinstalling Cluster Nodes</title>

  <para>
   To remove a role from a cluster, edit
   <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> and remove the
   corresponding line(s). Then run Stages 2 and 5 as described in
   <xref
    linkend="ceph.install.stack"/>.
  </para>

  <note>
   <title>Removing OSDs from Your Cluster</title>
   <para>
    In case you need to remove a particular OSD node from your cluster, ensure
    that your cluster has more free disk space than the disk you intend to
    remove. Bear in mind that removing an OSD results in rebalancing of the
    whole cluster.
   </para>
  </note>

  <para>
   When a role is removed from a minion, the objective is to undo all changes
   related to that role. For most of the roles, the task is simple, but there
   may be problems with package dependencies. If a package is uninstalled, its
   dependencies are not.
  </para>

  <para>
   Removed OSDs appear as blank drives. The related tasks overwrite the
   beginning of the file systems and remove backup partitions in addition to
   wiping the partition tables.
  </para>

  <note>
   <title>Preserving Partitions Created by Other Methods</title>
   <para>
    Disk drives previously configured by other methods, such as
    <command>ceph-deploy</command>, may still contain partitions. &deepsea;
    will not automatically destroy these. The administrator must reclaim these
    drives.
   </para>
  </note>

  <example xml:id="ex.ds.rmnode">
   <title>Removing a &sminion; from the Cluster</title>
   <para>
    If your storage minions are named, for example, 'data1.ceph', 'data2.ceph'
    ... 'data6.ceph', and the related lines in your
    <filename>policy.cfg</filename> are similar to the following:
   </para>
<screen>[...]
# Hardware Profile
profile-default/cluster/data*.sls
profile-default/stack/default/ceph/minions/data*.yml
[...]</screen>
   <para>
    Then to remove the &sminion; 'data2.ceph', change the lines to the
    following:
   </para>
<screen>
[...]
# Hardware Profile
profile-default/cluster/data[1,3-6]*.sls
profile-default/stack/default/ceph/minions/data[1,3-6]*.yml
[...]</screen>
   <para>
    Then run Stages 2 and 5:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.5</screen>
  </example>

  <example xml:id="ex.ds.mignode">
   <title>Migrating Nodes</title>
   <para>
    Assume the following situation: during the fresh cluster installation, you
    (the administrator) allocated one of the storage nodes as a stand-alone
    &ogw; while waiting for the gateway's hardware to arrive. Now the permanent
    hardware has arrived for the gateway and you can finally assign the
    intended role to the backup storage node and have the gateway role removed.
   </para>
   <para>
    After running Stages 0 and 1 (see <xref linkend="ds.depl.stages"/>) for the
    new hardware, you named the new gateway <literal>rgw1</literal>. If the
    node <literal>data8</literal> needs the &ogw; role removed and the storage
    role added, and the current <filename>policy.cfg</filename> looks like
    this:
   </para>
<screen># Hardware Profile
profile-default/cluster/data[1-7]*.sls
profile-default/stack/default/ceph/minions/data[1-7]*.sls

# Roles
role-rgw/cluster/data8*.sls</screen>
   <para>
    Then change it to:
   </para>
<screen># Hardware Profile
profile-default/cluster/data[1-8]*.sls
profile-default/stack/default/ceph/minions/data[1-8]*.sls

# Roles
role-rgw/cluster/rgw1*.sls</screen>
   <para>
    Run Stages 2 to 5. Stage 3 will add <literal>data8</literal> as a storage
    node. For a moment, <literal>data8</literal> will have both roles. Stage 4
    will add the &ogw; role to <literal>rgw1</literal> and Stage 5 will remove
    the &ogw; role from <literal>data8</literal>.
   </para>
  </example>
 </sect1>
 <sect1 xml:id="ds.mon">
  <title>Redeploying Monitor Nodes</title>

  <para>
   When one or more of your monitor nodes fail and are not responding, you need
   to remove the failed monitors from the cluster and possibly then re-add them
   back in the cluster.
  </para>

  <important>
   <title>The Minimum is Three Monitor Nodes</title>
   <para>
    The number of monitor nodes must not be less than three. If a monitor node
    fails, and as a result your cluster has only one or two monitor nodes, you
    need to temporarily assign the monitor role to other cluster nodes before
    you redeploy the failed monitor nodes. After you redeploy the failed
    monitor nodes, you can uninstall the temporary monitor roles.
   </para>
   <para>
    For more information on adding new nodes/roles to the &ceph; cluster, see
    <xref linkend="salt.adding.nodes"/> and
    <xref
     linkend="salt.adding.services"/>.
   </para>
   <para>
    For more information on removing cluster nodes, refer to
    <xref
     linkend="salt.node.removing"/>.
   </para>
  </important>

  <para>
   There are two basic degrees of a &ceph; node failure:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The &sminion; host is broken either physically or on the OS level, and
     does not respond to the <command>salt
     '<replaceable>minion_name</replaceable>' test.ping</command> call. In such
     case you need to redeploy the server completely by following the relevant
     instructions in <xref linkend="ceph.install.stack"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     The monitor related services failed and refuse to recover, but the host
     responds to the <command>salt '<replaceable>minion_name</replaceable>'
     test.ping</command> call. In such case, follow these steps:
    </para>
   </listitem>
  </itemizedlist>

  <procedure>
   <step>
    <para>
     Edit <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> on the
     &smaster;, and remove or update the lines that correspond to the failed
     monitor nodes so that they now point to the working monitor nodes.
    </para>
   </step>
   <step>
    <para>
     Run &deepsea; Stages 2 to 5 to apply the changes:
    </para>
<screen>
&prompt.smaster;<command>deepsea</command> stage run ceph.stage.2
&prompt.smaster;<command>deepsea</command> stage run ceph.stage.3
&prompt.smaster;<command>deepsea</command> stage run ceph.stage.4
&prompt.smaster;<command>deepsea</command> stage run ceph.stage.5
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt.node.add-disk">
  <title>Add an OSD to a Node</title>

  <para>
   To add a disk to an existing OSD node, verify that any partition on the disk
   was removed and wiped. Refer to <xref linkend="deploy.wiping.disk"/> in
   <xref
    linkend="ceph.install.stack"/> for more details. After the disk is
   empty, add the disk to the YAML file of the node. The path to the file is
   <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions/<replaceable>node_name</replaceable>.yml</filename>.
   After saving the file, run &deepsea; stages 2 and 3:
  </para>

<screen>&prompt.smaster;<command>deepsea</command> stage run ceph.stage.2
&prompt.smaster;<command>deepsea</command> stage run ceph.stage.3</screen>

  <tip>
   <title>Updated Profiles Automatically</title>
   <para>
    Instead of manually editing the YAML file, &deepsea; can create new
    profiles. To let &deepsea; create new profiles, the existing profiles have
    to be moved:
   </para>
<screen>&prompt.smaster;<command>old</command> /srv/pillar/ceph/proposals/profile-default/
&prompt.smaster;<command>deepsea</command> stage run ceph.stage.1
&prompt.smaster;<command>deepsea</command> stage run ceph.stage.2
&prompt.smaster;<command>deepsea</command> stage run ceph.stage.3</screen>
  </tip>
 </sect1>
 <sect1 xml:id="salt.removing.osd">
  <title>Removing an OSD</title>

  <para>
   A &osd; can be removed from the cluster by running the following command:
  </para>

<screen>&prompt.smaster;<command>salt-run</command> disengage.safety
&prompt.smaster;<command>salt-run</command> remove.osd <replaceable>OSD_ID</replaceable></screen>

  <para>
   <replaceable>OSD_ID</replaceable> has to be the number of the OSD without
   the term <literal>osd</literal>. For example, from <literal>osd.3</literal>
   only use the digit <literal>3</literal>.
  </para>

  <tip>
   <title>Removing Multiple OSDs</title>
   <para>
    There is no way to remove multiple OSDs in parallel with the
    <command>salt-run remove.osd</command> command. To automate the removal of
    multiple OSDs, you can use the following loop (5, 21, 33, 19 are ID numbers
    of the OSDs to be removed):
   </para>
<screen>
for i in 5 21 33 19
do
 echo $i
 salt-run disengage.safety
 salt-run remove.osd $i
done
</screen>
  </tip>

  <sect2 xml:id="osd.forced.removal">
   <title>Removing Broken OSDs Forcefully</title>
   <para>
    There are cases when removing an OSD gracefully (see
    <xref
     linkend="salt.removing.osd"/>) fails. This may happen for
    example if the OSD or its cache is broken, when it suffers from hanging I/O
    operations, or when the OSD disk fails to unmount. In such case, you need
    to force the OSD removal:
   </para>
<screen>&prompt.smaster;<replaceable>target</replaceable> osd.remove <replaceable>OSD_ID</replaceable> force=True</screen>
   <para>
    This command removes both the data partition, and the journal or WAL/DB
    partitions.
   </para>
   <para>
    To identify possibly orphaned journal/WAL/DB devices follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Pick the device that may have orphaned partitions and save the list of
      its partitions to a file:
     </para>
<screen>
&prompt.sminion;ls /dev/sdd?* > /tmp/partitions
</screen>
    </step>
    <step>
     <para>
      Run <command>readlink</command> against all block.wal, block.db, and
      journal devices, and compare the output to the previously saved list of
      partitions:
     </para>
<screen>
&prompt.sminion;readlink -f /var/lib/ceph/osd/ceph-*/{block.wal,block.db,journal} \
 | sort | comm -23 /tmp/partitions -
</screen>
     <para>
      The output is the list of partitions that are <emphasis>not</emphasis>
      used by &ceph;.
     </para>
    </step>
    <step>
     <para>
      Remove the orphaned partitions that do not belong to &ceph; with your
      preferred command (for example <command>fdisk</command>,
      <command>parted</command>, or <command>sgdisk</command>).
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="salt.automated.installation">
  <title>Automated Installation via &salt;</title>

  <para>
   The installation can be automated by using the &salt; reactor. For virtual
   environments or consistent hardware environments, this configuration will
   allow the creation of a &ceph; cluster with the specified behavior.
  </para>

  <warning>
   <para>
    &salt; cannot perform dependency checks based on reactor events. There is a
    real risk of putting your &smaster; into a death spiral.
   </para>
  </warning>

  <para>
   The automated installation requires the following:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A properly created
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>.
    </para>
   </listitem>
   <listitem>
    <para>
     Prepared custom configuration placed to the
     <filename>/srv/pillar/ceph/stack</filename> directory.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The default reactor configuration will only run Stages 0 and 1. This allows
   testing of the reactor without waiting for subsequent stages to complete.
  </para>

  <para>
   When the first salt-minion starts, Stage 0 will begin. A lock prevents
   multiple instances. When all minions complete Stage 0, Stage 1 will begin.
  </para>

  <para>
   If the operation is performed properly, change the last line in the
   <filename>/etc/salt/master.d/reactor.conf</filename>:
  </para>

<screen>- /srv/salt/ceph/reactor/discovery.sls</screen>

  <para>
   to
  </para>

<screen>- /srv/salt/ceph/reactor/all_stages.sls</screen>
 </sect1>
 <sect1 xml:id="deepsea.rolling_updates">
  <title>Updating the Cluster Nodes</title>

  <para>
   It is a good idea to apply rolling updates to your cluster's nodes
   regularly. To apply the updates, run Stage 0:
  </para>

<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>

  <para>
   If &deepsea; detects a running &ceph; cluster, it applies updates and
   restarts nodes sequentially. &deepsea; follows &ceph;'s official
   recommendation of first updating the monitors, then the OSDs, and lastly
   additional services, such as MDS, &rgw;, &igw;, or &ganesha;. &deepsea;
   stops the update process if it detects an issue in the cluster. A trigger
   for that can be:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     &ceph; reports 'HEALTH_ERR' for longer then 300 seconds.
    </para>
   </listitem>
   <listitem>
    <para>
     &sminion;s are queried for their assigned services to be still up and
     running after an update. The update fails if the services are down for
     more than 900 seconds.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Making these arrangements ensures that even with corrupted or failing
   updates, the &ceph; cluster is still operational.
  </para>

  <para>
   &deepsea; Stage 0 updates the system via <command>zypper update</command>
   and reboots the system if the kernel is updated. If you want to eliminate
   the possibility of a forced reboot of potentially all nodes, make sure that
   the latest kernel is installed and running before initiating &deepsea; Stage
   0.
  </para>

  <tip>
   <title><command>zypper patch</command></title>
   <para>
    If you prefer to update the system using the <command>zypper
    patch</command> command, edit
    <filename>/srv/pillar/ceph/stack/global.yml</filename> and add the
    following line:
   </para>
<screen>update_method_init: zypper-patch</screen>
  </tip>

  <para>
   You can change the default reboot behavior of &deepsea; Stage 0 by adding
   the following lines to the
   <filename>/srv/pillar/ceph/stack/global.yml</filename>:
  </para>

<screen>stage_prep_master: default-update-no-reboot
stage_prep_minion: default-update-no-reboot</screen>

  <para>
   <literal>stage_prep_master</literal> sets the Stage 0 behavior of the
   &smaster;, and <literal>stage_prep_minion</literal> sets the behavior of all
   minions. All available parameters are:
  </para>

  <variablelist>
   <varlistentry>
    <term>default</term>
    <listitem>
     <para>
      Install updates and reboot after updating.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-update-no-reboot</term>
    <listitem>
     <para>
      Install updates without rebooting.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-no-update-reboot</term>
    <listitem>
     <para>
      Reboots without installing updates.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-no-update-no-reboot</term>
    <listitem>
     <para>
      Do not install updates or reboot.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.salt.cluster.reboot">
  <title>Halting or Rebooting Cluster</title>

  <para>
   In some cases it may be necessary to halt or reboot the whole cluster. We
   recommended carefully checking for dependencies of running services. The
   following steps provide an outline for stopping and starting the cluster:
  </para>

  <procedure>
   <step>
    <para>
     Tell the &ceph; cluster not to mark OSDs as out:
    </para>
<screen>&prompt.root;<command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     Stop daemons and nodes in the following order:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Storage clients
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, for example &ganesha; or &rgw;
      </para>
     </listitem>
     <listitem>
      <para>
       &mds;
      </para>
     </listitem>
     <listitem>
      <para>
       &osd;
      </para>
     </listitem>
     <listitem>
      <para>
       &mgr;
      </para>
     </listitem>
     <listitem>
      <para>
       &mon;
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     If required, perform maintenance tasks.
    </para>
   </step>
   <step>
    <para>
     Start the nodes and servers in the reverse order of the shutdown process:
    </para>
    <orderedlist>
     <listitem>
      <para>
       &mon;
      </para>
     </listitem>
     <listitem>
      <para>
       &mgr;
      </para>
     </listitem>
     <listitem>
      <para>
       &osd;
      </para>
     </listitem>
     <listitem>
      <para>
       &mds;
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, for example &ganesha; or &rgw;
      </para>
     </listitem>
     <listitem>
      <para>
       Storage clients
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Remove the noout flag:
    </para>
<screen>&prompt.root;<command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ds.custom.cephconf">
  <title>Custom <filename>ceph.conf</filename> File</title>

  <para>
   If you need to put custom settings into the <filename>ceph.conf</filename>
   file, you can do so by modifying the configuration files in the
   <filename>/srv/salt/ceph/configuration/files/ceph.conf.d</filename>
   directory:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     global.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mon.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mgr.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mds.conf
    </para>
   </listitem>
   <listitem>
    <para>
     osd.conf
    </para>
   </listitem>
   <listitem>
    <para>
     client.conf
    </para>
   </listitem>
   <listitem>
    <para>
     rgw.conf
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title>Unique <filename>rgw.conf</filename></title>
   <para>
    The &ogw; offers a lot flexibility and is unique compared to the other
    <filename>ceph.conf</filename> sections. All other &ceph; components have
    static headers such as <literal>[mon]</literal> or
    <literal>[osd]</literal>. The &ogw; has unique headers such as
    <literal>[client.rgw.rgw1]</literal>. This means that the
    <filename>rgw.conf</filename> file needs a header entry. See
    <filename>/srv/salt/ceph/configuration/files/rgw.conf</filename> for an
    example.
   </para>
  </note>

  <important>
   <title>Run Stage 3</title>
   <para>
    After you make custom changes to the above mentioned configuration files,
    run Stage 3 to apply these changes to the cluster nodes:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.3</screen>
  </important>

  <para>
   These files are included from the
   <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>
   template file, and correspond to the different sections that the &ceph;
   configuration file accepts. Putting a configuration snippet in the correct
   file enables &deepsea; to place it into the correct section. You do not need
   to add any of the section headers.
  </para>

  <tip>
   <para>
    To apply any configuration options only to specific instances of a daemon,
    add a header such as <literal>[osd.1]</literal>. The following
    configuration options will only be applied to the OSD daemon with the ID 1.
   </para>
  </tip>

  <sect2>
   <title>Overriding the Defaults</title>
   <para>
    Later statements in a section overwrite earlier ones. Therefore it is
    possible to override the default configuration as specified in the
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>
    template. For example, to turn off cephx authentication, add the following
    three lines to the
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename>
    file:
   </para>
<screen>auth cluster required = none
auth service required = none
auth client required = none</screen>
  </sect2>

  <sect2>
   <title>Including Configuration Files</title>
   <para>
    If you need to apply a lot of custom configurations, use the following
    include statements within the custom configuration files to make file
    management easier. Following is an example of the
    <filename>osd.conf</filename> file:
   </para>
<screen>[osd.1]
{% include "ceph/configuration/files/ceph.conf.d/osd1.conf" ignore missing %}
[osd.2]
{% include "ceph/configuration/files/ceph.conf.d/osd2.conf" ignore missing %}
[osd.3]
{% include "ceph/configuration/files/ceph.conf.d/osd3.conf" ignore missing %}
[osd.4]
{% include "ceph/configuration/files/ceph.conf.d/osd4.conf" ignore missing %}</screen>
   <para>
    In the previous example, the <filename>osd1.conf</filename>,
    <filename>osd2.conf</filename>, <filename>osd3.conf</filename>, and
    <filename>osd4.conf</filename> files contain the configuration options
    specific to the related OSD.
   </para>
   <tip>
    <title>Runtime Configuration</title>
    <para>
     Changes made to &ceph; configuration files take effect after the related
     &ceph; daemons restart. See <xref linkend="ceph.config.runtime"/> for more
     information on changing the &ceph; runtime configuration.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.config.runtime">
  <title>Runtime &ceph; Configuration</title>

  <para>
   <xref linkend="ds.custom.cephconf"/> describes how to make changes to the
   &ceph; configuration file <filename>ceph.conf</filename>. However, the
   actual cluster behavior is determined not by the current state of the
   <filename>ceph.conf</filename> file but by the configuration of the running
   &ceph; daemons, which is stored in memory.
  </para>

  <para>
   You can query an individual &ceph; daemon for a particular configuration
   setting using the <emphasis>admin socket</emphasis> on the node where the
   daemon is running. For example, the following command gets the value of the
   <option>osd_max_write_size</option> configuration parameter from daemon
   named <literal>osd.0</literal>:
  </para>

<screen>&prompt.root;ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok \
config get osd_max_write_size
{
  "osd_max_write_size": "90"
}</screen>

  <para>
   You can also <emphasis>change</emphasis> the daemons' settings at runtime.
   Remember that this change is temporary and will be lost after the next
   daemon restart. For example, the following command changes the
   <option>osd_max_write_size</option> parameter to '50' for all OSDs in the
   cluster:
  </para>

<screen>&prompt.root;ceph tell osd.* injectargs --osd_max_write_size 50</screen>

  <warning>
   <title><command>injectargs</command> is Not Reliable</title>
   <para>
    Unfortunately, changing the cluster settings with the
    <command>injectargs</command> command is not 100% reliable. If you need to
    be sure that the changed parameter is active, change it in the
    configuration file and restart all daemons in the cluster.
   </para>
  </warning>
 </sect1>
</chapter>
